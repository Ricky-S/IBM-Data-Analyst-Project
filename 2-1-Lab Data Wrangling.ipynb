{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf7d3fd-e26e-4bd3-a976-4876fca00ee1",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8adf17-b722-411a-97f0-3dd26aec199f",
   "metadata": {},
   "source": [
    "# **Data Wrangling Lab**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989614d2-d8dc-45cd-a0fa-7de8def57de2",
   "metadata": {},
   "source": [
    "Estimated time needed: **45 to 60** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f90607f-4836-4dee-9d0b-6cb182209cae",
   "metadata": {},
   "source": [
    "In this lab, you will perform data wrangling which involves transforming and preparing raw data into a structured and usable format for analysis. It involves various techniques to clean, normalize, and integrate data from different sources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea74a7fe-c6b7-4036-a8c9-7acfb1048212",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b5ea3e-5dc0-4381-9c06-20afbf20aea0",
   "metadata": {},
   "source": [
    "After completing this lab, you will be able to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865c61d0-c19c-4970-9605-9350de6b4899",
   "metadata": {},
   "source": [
    "-   Apply techniques to identify and remove duplicate values from the given dataset.\n",
    "  \n",
    "-   Identify missing values in the dataset.\n",
    "\n",
    "-   Use appropriate imputation strategies to handle missing values.\n",
    "\n",
    "-   Use normalization techniques to prepare data for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c1be4b-60ce-4302-9ba5-a13d4f23c5e3",
   "metadata": {},
   "source": [
    "## Load the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351688f2-808e-4a6b-813f-780b7136b3c0",
   "metadata": {},
   "source": [
    "#### Step 1: Import the necessary module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7427eb-3ce0-4fec-9e2b-d1ea65026ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e3a49-9e56-4cbb-9c8c-d2662f6d4c31",
   "metadata": {},
   "source": [
    "#### Import the library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5283111-b1dc-4e8c-ab85-7aa684359ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9ccca6-9c9c-4bb4-a9ae-98623d3e289e",
   "metadata": {},
   "source": [
    "#### Step 2: Load the dataset into a dataframe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56875436-0dfa-4f3a-8748-38208315e33b",
   "metadata": {},
   "source": [
    "You can load the Stack Overflow survey data using the <code>df = pd.read_csv(file_name)</code>\n",
    "\n",
    "Replace the URL with the path to the dataset if it is available. For now, we use a placeholder link for illustration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a67cf79-722c-44ba-b4e7-c3ea3a154e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Stack Overflow survey data from the provided URL\n",
    "dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv\"\n",
    "df = pd.read_csv(dataset_url)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd533e4-c71e-4242-8ff7-a72ba63d85fe",
   "metadata": {},
   "source": [
    "#### Step 3: Finding and Removing Duplicates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df2a6b4-8ee1-4de2-8c4f-7e6c9c082574",
   "metadata": {},
   "source": [
    "**3.1 Find Duplicate Values**\n",
    "\n",
    "In this section, you will identify duplicate rows in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0aef53-d880-4f1c-abb0-afa47539158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "# Find duplicate rows in the DataFrame\n",
    "duplicates = df[df.duplicated()]\n",
    "\n",
    "# Display duplicate rows\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911f7726-de4e-4b2c-9761-bcd7006d6ae2",
   "metadata": {},
   "source": [
    "**3.2 Remove Duplicate Values**\n",
    "\n",
    "Remove all duplicate rows from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137b01ed-3a33-45fa-a514-45cb8f197b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "# Remove duplicate rows from the DataFrame\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d255a8-6832-4f93-af04-9700aeaeb69e",
   "metadata": {},
   "source": [
    "#### Step 4: Finding and Imputing Missing Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35209416-08ab-409d-94ae-22240f7fe187",
   "metadata": {},
   "source": [
    "**4.1 Finding Missing Values**\n",
    "\n",
    "Check for missing values across all columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd181f53-4a9e-44ac-aa3b-36a400d6553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "# Count missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Display the count of missing values per column\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d85943-4b00-4d41-8462-2171e40cf13f",
   "metadata": {},
   "source": [
    "**4.2 Handling Missing Values for a Specific Column**\n",
    "                           \n",
    "Check for missing values in the RemoteWork column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RemoteWork']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600bcea4-6d09-4d3f-9d84-10629a0aec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "missing_count = df['RemoteWork'].isnull().sum()\n",
    "print(\"Missing values in column:\", missing_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afba8140-09e3-42da-929e-8c57f3a3067d",
   "metadata": {},
   "source": [
    "**4.3 Impute Missing Values**\n",
    "\n",
    "Impute missing values in `RemoteWork` with the most frequent value:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756c19c9-f7d2-46bf-b6a2-4317b1d8c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "test = df['RemoteWork'].value_counts()\n",
    "test.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0383e-f3d2-4936-94ee-5495e0b18466",
   "metadata": {},
   "source": [
    "#### Step 5: Normalizing Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db88442-43ed-4516-b999-518082ca194d",
   "metadata": {},
   "source": [
    "For this task, we will work with the `CompFreq` and `CompTotal` columns to create a standardized `NormalizedAnnualCompensation` column.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63a48b6-a168-44fe-b500-43f3016a0ee1",
   "metadata": {},
   "source": [
    "**5.1 Analyze the Annual Compensation Data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279fa2dc-e3af-4ea9-89bf-4bb765be6a6e",
   "metadata": {},
   "source": [
    "Examine the `ConvertedCompYearly` column to understand the range of annual compensation reported by respondents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bee587-96fe-4ed4-b3d8-45730203e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "data_column = df['ConvertedCompYearly']\n",
    "data_column.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6717c4a9-e8de-4995-8b6d-e1f7912b22bf",
   "metadata": {},
   "source": [
    "**5.2 Normalize the Annual Compensation for Comparative Analysis**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e27f25-5d0b-4cef-a138-0ba52dd3d0b4",
   "metadata": {},
   "source": [
    "Using Min-Max Scaling or Z-score normalization, create a `NormalizedAnnualCompensation` column that standardizes the `ConvertedCompYearly` data. This makes it easier to compare compensation levels within a common range or around a mean.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ec1a79-e4d7-46b9-b83f-1e5cf140994e",
   "metadata": {},
   "source": [
    "- Option 1: Remove Rows with `NaN` in `ConvertedCompYearly`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1966344f-1714-4921-95c4-661a3139eda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3235702-8864-4286-9014-56cd3c834c72",
   "metadata": {},
   "source": [
    "- Option 2: Fill `NaN` with a Placeholder Value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b251280-f3c0-45fc-8dba-7803746b242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70d0883-7f8a-4181-885b-a9d684aa0c87",
   "metadata": {},
   "source": [
    "### Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7812855-508f-4693-942f-e3b7a16422a4",
   "metadata": {},
   "source": [
    "In this lab, you successfully:\n",
    "\n",
    "Identified and removed duplicate rows.\n",
    "Found and imputed missing values.\n",
    "Normalized compensation data to create an easily comparable metric.\n",
    "You can extend these tasks to other columns in the dataset to deepen your data wrangling skills.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dd4393-f9b6-4a42-9e99-171857c513b9",
   "metadata": {},
   "source": [
    "<!--## Change Log\n",
    "\n",
    "| Date (YYYY-MM-DD) | Version | Changed By | Change Description           |\n",
    "| ----------------- | ------- | ---------- | ---------------------------- |\n",
    "| 2024-11-05        | 3.0     | Madhusudan Moole | Updated Lab                  |\n",
    "| 2024-05-25        | 2.0     | Madhusudan Moole | ID reviewed                  |\n",
    "| 2023-09-18        | 1.0     | Raghul Ramesh    | Created lab   |        |\n",
    "\n",
    "--!>\n",
    "\n",
    "## <h3 align=\"center\"> © IBM Corporation. All rights reserved. <h3/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c91ba8-cba5-45e2-932a-838cccfd7e32",
   "metadata": {},
   "source": [
    "## <h3 align=\"center\"> © IBM Corporation. All rights reserved. <h3/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibm310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "prev_pub_hash": "0f04a21e9f36512f23a10dc3761e4f9dbc457648c3934db412c690e44c3d4ba1"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
